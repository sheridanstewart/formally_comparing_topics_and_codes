{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from matplotlib import rcParams\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upper_bound(num_tests = 1, alpha = 0.05, one_tailed = False):\n",
    "    \"\"\"Helper function for implementing bootstrap percentile method\"\"\"\n",
    "    lower = alpha/num_tests/2\n",
    "    if one_tailed:\n",
    "        lower *= 2\n",
    "    return 1 - lower\n",
    "\n",
    "\n",
    "def get_upper_index(upper_bound, distribution):\n",
    "    \"\"\"Find index based on upper bound and distribution size\"\"\"\n",
    "    upper_idx = int(np.floor(upper_bound * len(distribution)))\n",
    "    return upper_idx\n",
    "\n",
    "\n",
    "def tjur_r2_ALL(X_df, y_df):\n",
    "    \"\"\"\n",
    "    Tjur, T. (2009). Coefficients of determination in logistic regression \n",
    "    modelsâ€”a new proposal: The coefficient of discrimination. The American \n",
    "    Statistician, 63(4),366-372. DOI: 10.1198/tast.2009.08210\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    r2s = []\n",
    "    y_cols = y_df.columns\n",
    "    X_cols = X_df.columns\n",
    "    X = X_df[X_cols].to_numpy()\n",
    "    for y_col in y_cols:\n",
    "        y = y_df[y_col].values\n",
    "        r2_row = []\n",
    "        for x_col in X_cols:\n",
    "            X = X_df[x_col].values.reshape(-1,1)\n",
    "            logit = LogisticRegression(penalty='none', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, max_iter=2000, multi_class='auto', verbose=0).fit(X, y)\n",
    "            yhat = logit.predict_proba(X)\n",
    "            yhat = [yh[1] for yh in yhat]\n",
    "            response = pd.DataFrame(list(zip(y, yhat)), columns=[\"y\", \"yhat\"])\n",
    "            r2 = np.mean(response[response[\"y\"]==True][\"yhat\"]) - np.mean(response[response[\"y\"]==False][\"yhat\"])\n",
    "            r2_row.append(r2)\n",
    "            count += 1\n",
    "        X = X_df[X_cols].to_numpy()\n",
    "        logit = LogisticRegression(penalty='none', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, max_iter=2000, multi_class='auto', verbose=0).fit(X, y)\n",
    "        yhat = logit.predict_proba(X)\n",
    "        yhat = [yh[1] for yh in yhat]\n",
    "        response = pd.DataFrame(list(zip(y, yhat)), columns=[\"y\", \"yhat\"])\n",
    "        r2_full = np.mean(response[response[\"y\"]==True][\"yhat\"]) - np.mean(response[response[\"y\"]==False][\"yhat\"])\n",
    "        r2_row.append(r2_full)\n",
    "        r2s.append(r2_row)\n",
    "    return np.array(r2s)\n",
    "\n",
    "\n",
    "def hypothesis_testing_r2(real_ests, sim_ests_dict, alpha = 0.05, bonferroni = True, one_tailed = False):\n",
    "    \"\"\"\n",
    "    Note: Relaxed assumption that test should be against simulated values in cell_ij only.\n",
    "    Tests are against all Tjur R2s for code_i: increased by a factor of k.\n",
    "    Excludes last value (estimate from regressing code on full topic model) for pairwise\n",
    "    hypothesis tests.\n",
    "    \"\"\"\n",
    "    num_codes, num_cols = real_ests.shape\n",
    "    test_mat = np.zeros((num_codes, num_cols))\n",
    "    num_topics = num_cols - 1\n",
    "    num_tests = 1\n",
    "    if bonferroni == True:\n",
    "        num_tests = real_ests.size\n",
    "    upper_bound = get_upper_bound(num_tests, alpha, one_tailed)\n",
    "    # pairwise tests (excluding last columns):\n",
    "    for i, row in enumerate(real_ests):\n",
    "        sims = [k15_dict[i][k] for k in range(15)] # pairwise, excludes last column\n",
    "        sims = sorted([sim for cell in sims for sim in cell])\n",
    "        upper_idx = get_upper_index(upper_bound, sims)\n",
    "        test_stat = sims[upper_idx]\n",
    "        for j, col in enumerate(row[:-1]): # pairwise, excluding last column\n",
    "            real_r2 = real_ests[i][j]\n",
    "            if real_r2 > test_stat:\n",
    "                test_mat[i][j] = 1\n",
    "    print(np.count_nonzero(test_mat)/test_mat.size)\n",
    "    # tests on full models:\n",
    "    for i, row in enumerate(real_ests):\n",
    "        full_model_r2 = real_ests[i][-1]\n",
    "        last_key = sorted(list(sim_ests_dict[i]))[-1]\n",
    "        sims = sorted(sim_ests_dict[i][last_key]) # last column, 1000 simulations\n",
    "        assert len(sims) == 1000\n",
    "        upper_idx = get_upper_index(upper_bound, sims)\n",
    "        test_stat = sims[upper_idx]\n",
    "        if full_model_r2 > test_stat:\n",
    "            test_mat[i][-1] = 1\n",
    "    print(np.count_nonzero(test_mat)/test_mat.size)\n",
    "    return test_mat\n",
    "\n",
    "\n",
    "def filter_columns(sig_test_mat_):\n",
    "    \"\"\"Identify columns to exclude if filter_cols == False\"\"\"\n",
    "    idx_to_filter = []\n",
    "    num_cols = sig_test_mat_.shape[1]\n",
    "    for i in range(num_cols):\n",
    "        col = sig_test_mat_[:, i]\n",
    "        sum_ = sum(col)\n",
    "        if sum_ == 0:\n",
    "            idx_to_filter.append(i)\n",
    "    return idx_to_filter\n",
    "    \n",
    "\n",
    "def results_to_strings(results, sig_tests, filter_cols=False):\n",
    "    \"\"\"Add asterisks to significant results (etc.)\"\"\"\n",
    "    mat = []\n",
    "    for i, row in enumerate(results):\n",
    "        row_str = []\n",
    "        for j, cell in enumerate(row):\n",
    "            res = f\"{cell:.3f}\"\n",
    "            if sig_tests[i][j]:\n",
    "                res += \"*\"\n",
    "            row_str.append(res)\n",
    "        mat.append(row_str)\n",
    "    mat = np.array(mat)\n",
    "    col_names = [f\"Topic {i+1}\" for i in range(sig_tests.shape[1])]\n",
    "    if filter_cols:\n",
    "        idx_to_filter = filter_columns(sig_tests)\n",
    "        mat = np.delete(mat, idx_to_filter, 1)\n",
    "        col_names = list(np.delete(col_names, idx_to_filter))\n",
    "    return mat, col_names\n",
    "\n",
    "\n",
    "def remove_star(string_: str) -> float:\n",
    "    \"\"\"Convert output of results_to_strings() to floats\"\"\"\n",
    "    return float(string_.replace(\"*\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k15_dict = pickle.load(open(\"simulations_tjur_r2_k15.d\", \"rb\"))\n",
    "k35_dict = pickle.load(open(\"simulations_tjur_r2_k35.d\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(k15_dict[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_k15 = pd.read_csv(\"df_for_comparison_k15.csv\")\n",
    "df_k35 = pd.read_csv(\"df_for_comparison_k35.csv\")\n",
    "\n",
    "topic_cols = [f\"topic_{i}\" for i in range(1,36)]\n",
    "\n",
    "df_k15 = df_k15[topic_cols[:15]]\n",
    "df_k35 = df_k35[topic_cols[:35]]\n",
    "\n",
    "df_qual = pd.read_csv(\"df_for_comparison_k15.csv\")\n",
    "cols = list(df_qual.columns)[13:-18]\n",
    "cols.remove(\"Index Codes Applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qual.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_cols = ['Academic Medicine Applied',\n",
    " 'Culture of medicine Applied',\n",
    " 'Expectations Applied',\n",
    " 'Hospital/clinic hours and environment Applied',\n",
    " 'Incentive/payment structure Applied',\n",
    " 'Interpersonal Applied',\n",
    " 'Job changes Applied',\n",
    " 'Medical training Applied',\n",
    " 'Missed opportunities Applied',\n",
    " 'Pay/Compensation Applied',\n",
    " 'Psychological Applied',\n",
    " 'Sub-specialities Applied',\n",
    " 'Great quote/example Applied',\n",
    " 'Motherhood Specific Applied',\n",
    " 'Motherhood Specific/Breastfeeding/Pumping Applied',\n",
    " 'Motherhood Specific/Childcare/Household challenges Applied',\n",
    " 'Motherhood Specific/Family leave Applied']\n",
    "\n",
    "assert cols == orig_cols\n",
    "\n",
    "qual_codes = [code.replace(\" Applied\", \"\") for code in cols]\n",
    "\n",
    "df_qual = df_qual[cols]\n",
    "\n",
    "display(df_qual.head())\n",
    "\n",
    "display(df_k15.head())\n",
    "\n",
    "display(df_k35.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_qual.shape)\n",
    "print(df_k15.shape)\n",
    "print(df_k35.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(map(lambda x: round(x, 6), df_k15.sum(axis=1)))) # checking that probabilities sum to approximately 1\n",
    "print(set(map(lambda x: round(x, 6), df_k35.sum(axis=1)))) # checking that probabilities sum to approximately 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tjur_k15 = tjur_r2_ALL(df_k15, df_qual)\n",
    "tjur_k35 = tjur_r2_ALL(df_k35, df_qual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tjur_k15.shape, tjur_k35.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tjur_k15.shape[0] == len(k15_dict.keys())\n",
    "assert tjur_k15.shape[1] == len(k15_dict[0].keys())\n",
    "assert tjur_k35.shape[0] == len(k35_dict.keys())\n",
    "assert tjur_k35.shape[1] == len(k35_dict[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tjur_k15.shape[0] * tjur_k15.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tests = 1\n",
    "interval = 0.95\n",
    "interval = 1 - (1 - interval)/num_tests\n",
    "lower = (1 - interval)/2 * 100\n",
    "upper = 100 - lower\n",
    "print(f\"95% confidence level without correction for multiple testing: {interval:.2f}\")\n",
    "print(f\"Percentile of upper bound: {upper:.1f}\", \"\\n\")\n",
    "\n",
    "num_tests = tjur_k15.shape[0] * tjur_k15.shape[1]\n",
    "interval = 0.95\n",
    "interval = 1 - (1 - interval)/num_tests\n",
    "lower = (1 - interval)/2 * 100\n",
    "upper = 100 - lower\n",
    "print(f\"95% confidence level with correction for multiple testing ({num_tests} tests): {interval:.6f}\")\n",
    "print(f\"Percentile of upper bound: {upper:.5f}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k15_dict.keys() # indices for the 17 qualitative codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(k15_dict[0]) # estimates for regressing code 0 on each of the 15 topics and then on all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(k15_dict[0][0]) # estimates assuming r2_ij should be compared to cell_ij in results of simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tjur_k15.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3. Coefficients of discrimination between codes and topics in preferred model (k = 15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = [\"Transitions\", \"Family Leave\", \"Promotion Inequality\", \"Power\", \"Burnout\", \"Unequal Compensation\", \n",
    "                \"Psychosocial Support\", \"Respect\", \"Training\", \"Staff Interactions\", \"Career Advancement\", \n",
    "                \"Favoritism\", \"Hierarchy\", \"Agency\", \"Pumping\", \"Full Model\"]\n",
    "\n",
    "sig_tests15 = hypothesis_testing_r2(tjur_k15, k15_dict, alpha = 0.05, bonferroni = True, one_tailed = False)\n",
    "mat, _ = results_to_strings(tjur_k15, sig_tests15)\n",
    "df = pd.DataFrame(mat, columns=topic_labels)\n",
    "df.insert(0, \"Code\", qual_codes)\n",
    "\n",
    "outf = \"pref_model_table.csv\"\n",
    "df.to_csv(outf)\n",
    "\n",
    "mask = np.invert(sig_tests15.astype(bool))\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "cmap = sns.color_palette(\"Blues\", n_colors=1000)\n",
    "rcParams['figure.figsize'] = 12,9\n",
    "rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "plot = sns.heatmap(tjur_k15, vmin=0.0, vmax=1.0, center=0.4,\n",
    "            square=False, linewidths=0.01,\n",
    "            linecolor=\"lightgray\",\n",
    "            yticklabels=qual_codes,\n",
    "            xticklabels=topic_labels,\n",
    "            annot=True,\n",
    "            fmt=\".3f\",\n",
    "            cmap = cmap,\n",
    "            mask = mask)\n",
    "\n",
    "plot.set_xticklabels(plot.get_xticklabels(), rotation=45, horizontalalignment=\"right\")\n",
    "plt.tight_layout()\n",
    "for i in range(tjur_k15.shape[1]+1):\n",
    "    plt.axvline(x=i, color=\"lightgray\")\n",
    "for i in range(18): # ?\n",
    "    plt.axhline(y=i, color=\"lightgray\")\n",
    "\n",
    "plt.savefig(\"comparing_topic_models_qual_coding_figure3.png\", format=\"png\", transparent=False, dpi=600)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure 3. Coefficients of discrimination between codes and topics in preferred model (k = 15).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 6. Coefficients of discrimination between codes and topics in alternative model (k = 35)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topic_labels = [\"Transitions\", \"Family Leave\", \"Promotion Inequality\", \"Power\", \"Burnout\", \"Unequal Compensation\", \n",
    "                \"Psychosocial Support\", \"Respect\", \"Training\", \"Staff Interactions\", \"Career Advancement\", \n",
    "                \"Favoritism\", \"Hierarchy\", \"Agency\", \"Pumping\", \"Full Model\"]\n",
    "\n",
    "sig_tests35 = hypothesis_testing_r2(tjur_k35, k35_dict, alpha = 0.05, bonferroni = True, one_tailed = False)\n",
    "mat, _ = results_to_strings(tjur_k35, sig_tests35)\n",
    "df = pd.DataFrame(mat, columns=[f\"Topic {i}\" for i in range(1,36)] + [\"Full Model\"])\n",
    "df.insert(0, \"Code\", qual_codes)\n",
    "\n",
    "outf = \"alt_model_table.csv\"\n",
    "df.to_csv(outf)\n",
    "\n",
    "mask = np.invert(sig_tests35.astype(bool))\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "f, ax = plt.subplots(figsize=(20, 9))\n",
    "\n",
    "cmap = sns.color_palette(\"Blues\", n_colors=1000)\n",
    "rcParams['figure.figsize'] = 16,9\n",
    "rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "plot = sns.heatmap(tjur_k35, vmin=0.0, vmax=1.0, center=0.4,\n",
    "            square=False, linewidths=0.01,\n",
    "            linecolor=\"lightgray\",\n",
    "            yticklabels=qual_codes,\n",
    "            xticklabels=[f\"Topic {i}\" for i in range(1,36)] + [\"Full Model\"],\n",
    "            annot=True,\n",
    "            fmt=\".3f\",\n",
    "            cmap = cmap,\n",
    "            mask = mask)\n",
    "\n",
    "plot.set_xticklabels(plot.get_xticklabels(), rotation=45, horizontalalignment=\"right\")\n",
    "plt.tight_layout()\n",
    "for i in range(tjur_k35.shape[1]+1):\n",
    "    plt.axvline(x=i, color=\"lightgray\")\n",
    "for i in range(18): # ?\n",
    "    plt.axhline(y=i, color=\"lightgray\")\n",
    "\n",
    "plt.savefig(\"comparing_topic_models_qual_coding_figure6.png\", format=\"png\", transparent=False, dpi=600)\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure 6. Coefficients of discrimination between codes and topics in alternative model (k = 35).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 3. Explanation of variance in qualitative codes by preferred (k = 15) and alternative (k = 35) LDA topic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pref_model = pd.read_csv(\"pref_model_table.csv\")\n",
    "alt_model = pd.read_csv(\"alt_model_table.csv\")\n",
    "pref = pref_model[[\"Code\", \"Full Model\"]].rename(columns={\"Full Model\": \"Preferred Model (Tjur R2)\"})\n",
    "alt = alt_model[[\"Code\", \"Full Model\"]].rename(columns={\"Full Model\": \"Alternative Model (Tjur R2)\"})\n",
    "\n",
    "comp_tab = pref.merge(alt, on = \"Code\")\n",
    "comp_tab[\"R2 Ratio\"] = comp_tab.apply(lambda row: remove_star(row[\"Preferred Model (Tjur R2)\"])/remove_star(row[\"Alternative Model (Tjur R2)\"]), axis = 1)\n",
    "comp_tab[\"R2 Ratio\"] = comp_tab[\"R2 Ratio\"].apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "comp_tab.to_csv(\"comparison_table.csv\", index=None)\n",
    "\n",
    "print(\"Table 3. Explanation of variance in qualitative codes by preferred (k = 15) and alternative (k = 35) LDA topic models.\")\n",
    "\n",
    "comp_tab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
