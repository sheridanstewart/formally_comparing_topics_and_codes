{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5028eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as spacy_stopwords\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5cce76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_strings(doc):\n",
    "    \"\"\"Preliminary cleaning of responses to open-ended questions\"\"\"\n",
    "    text_ = doc.split(\",\")[1:]\n",
    "    text_as_string = \" \".join(text_).replace(\"Question: disc_exp; Answer: \", \"\")\n",
    "    text_as_string = text_as_string.strip().strip(\"'\").strip('\"').replace(\"\\n\", \" \")\n",
    "    return text_as_string\n",
    "\n",
    "\n",
    "def make_df(docs):\n",
    "    \"\"\"Create a dataframe while calling return_strings() on text\"\"\"\n",
    "    if \"Media Title\" in docs[0]:\n",
    "        docs = docs[1:]\n",
    "    doc_ids = []\n",
    "    doc_texts = []\n",
    "    for doc in docs:\n",
    "        doc_id = doc.split(\",\")[0]\n",
    "        doc_text = return_strings(doc)\n",
    "        doc_ids.append(doc_id)\n",
    "        doc_texts.append(doc_text)\n",
    "    docs_ = list(zip(doc_ids, doc_texts))\n",
    "    docs_ = [doc_ for doc_ in docs_ if doc_[1]]\n",
    "    cols_ = [\"doc_id\", \"text\"]\n",
    "    df = pd.DataFrame(docs_, columns=cols_)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_stop_words(nlp):\n",
    "    \"\"\"Merge stopword lists from NLTK and spaCy\"\"\"\n",
    "    nltk_sw = set(nltk_stopwords.words(\"english\"))\n",
    "    stop_words = spacy_stopwords.union(nltk_sw)\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def fix_ordinal_nums(word):\n",
    "    \"\"\"Normalize ordinal numbers in strings\"\"\"\n",
    "    ord_num_reg = \"\\d+[(st)(nd)(rd)(th)]\"\n",
    "    try:\n",
    "        if any(re.findall(ord_num_reg, word)):\n",
    "            word = re.sub(\"[(st)(nd)(rd)(th)]\", \"\", word)\n",
    "            word = num2words(word, lang=\"en\", to=\"ordinal\")\n",
    "        return word\n",
    "    except:\n",
    "        return word\n",
    "\n",
    "\n",
    "pos_to_remove = set([\"NUM\", \"PUNCT\", \"SYM\", \"X\", \"CCONJ\", \"DET\", \"ADP\"])\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = get_stop_words(nlp)\n",
    "\n",
    "\n",
    "def preprocess(response, stop_words=stop_words, nlp=nlp, pos_to_remove=pos_to_remove, keep_pronouns=False):\n",
    "    \"\"\"Main function for preprocessing. See Table 1 and description.\"\"\"\n",
    "    response = unidecode(response)\n",
    "    response = \" \".join([w for word in response.lower().split() for w in contractions.fix(word).split()])\n",
    "    response = \" \".join([fix_ordinal_nums(word) for word in response.split()])\n",
    "    response = nlp(response)\n",
    "    if keep_pronouns:\n",
    "        response = [word.lemma_ if word.pos_ != \"PRON\" else word.text for word in response if ((word.text not in stop_words) and (word.pos_ not in pos_to_remove))]\n",
    "    else:\n",
    "        response = [word.lemma_ for word in response if ((word.text not in stop_words) and (word.lemma_ not in stop_words) and (word.pos_ not in pos_to_remove))]\n",
    "    response = \" \".join(response)\n",
    "    response = re.sub(\"[^a-z]\", \" \", response)\n",
    "    response = re.sub(\"\\s+\", \" \", response)\n",
    "    if response:\n",
    "        return response.split()\n",
    "    return None\n",
    "\n",
    "\n",
    "def train_ngram_model(corpus_, min_count=5, threshold=100, inc_trigrams=True):\n",
    "    \"\"\"Identify ngrams in the corpus and replace them with versions connected by underscores\"\"\"\n",
    "    corpus_ = corpus_[corpus_[\"cleaned\"] != \"\"]\n",
    "    original_ = corpus_[\"original\"]\n",
    "    preprocessed_ = [p.split() for p in corpus_[\"cleaned\"]]\n",
    "    bigram_model = Phraser(Phrases(preprocessed_, min_count=min_count, threshold=threshold))\n",
    "    ngrams = bigram_model[preprocessed_]\n",
    "    ngrams = list(ngrams)\n",
    "    if inc_trigrams:\n",
    "        trigram_model = Phraser(Phrases(ngrams, min_count=min_count, threshold=threshold))\n",
    "        ngrams = trigram_model[ngrams]\n",
    "        ngrams = list(ngrams)\n",
    "    corpus_[\"ngrams\"] = ngrams\n",
    "    for n in ngrams:\n",
    "        if \"_\" in ngrams:\n",
    "            print(n)\n",
    "    return corpus_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9e1a0",
   "metadata": {},
   "source": [
    "## Effects of Preprocessing Steps (See Table 1 in Paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345b9184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Told 'pregnancy was a choice...not an illness' so shouldn't be allowed to use sick leave for maternity leave.\n",
      "\n",
      "Lowercase: told 'pregnancy was a choice...not an illness' so shouldn't be allowed to use sick leave for maternity leave.\n",
      "\n",
      "Expanded contractions: told 'pregnancy was a choice...not an illness' so should not be allowed to use sick leave for maternity leave.\n",
      "\n",
      "Everything but ngrams: tell pregnancy choice illness allow use sick leave maternity leave\n"
     ]
    }
   ],
   "source": [
    "example = \"Told 'pregnancy was a choice...not an illness' so shouldn't be allowed to use sick leave for maternity leave.\"\n",
    "print(f\"Original: {example}\\n\")\n",
    "\n",
    "print(f\"Lowercase: {example.lower()}\\n\")\n",
    "\n",
    "example = \" \".join([w for word in example.lower().split() for w in contractions.fix(word).split()])\n",
    "print(f\"Expanded contractions: {example}\\n\")\n",
    "\n",
    "print(f\"Everything but ngrams: {' '.join(preprocess(example))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81ae51d",
   "metadata": {},
   "source": [
    "## Preprocessing the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = open(\"PMG 2016 Discrimination Data Excerpts All-original data 1.14.20_utf_8.csv\", \"r\", encoding=\"utf-8\").read().split(\"\\n\")\n",
    "df = make_df(docs)\n",
    "responses = df[\"text\"].values\n",
    "responses = [response.strip().strip(\"'\").strip('\"') for response in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a3fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = [preprocess(response) for response in responses]\n",
    "\n",
    "assert len(preprocessed) == len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_titles = df[\"doc_id\"].values\n",
    "IDs = list(range(len(responses)))\n",
    "d = {\"media_title\":media_titles, \"doc_id\":IDs, \"original\":responses, \"cleaned\":[\" \".join(p) if p else \"\" for p in preprocessed]}\n",
    "\n",
    "df = pd.DataFrame(d).drop_duplicates(subset=\"original\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b843b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = train_ngram_model(df, min_count=5, threshold=0.1)\n",
    "ngrams[\"text\"] = [\" \".join(n) for n in ngrams[\"ngrams\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"ngrams_df.csv\"\n",
    "ngrams.to_csv(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
